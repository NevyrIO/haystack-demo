{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI with Haystack on Azure \n",
    "This notebook shows an example of how to integrate AI models with your existing data on Azure. \n",
    "\n",
    "### Steps: \n",
    "* Access pdf data from Azure storage\n",
    "* Extract text and store in vector database (Opensearch)\n",
    "* Show basic Haystack flow\n",
    "* Prompt with open source AI model (LLM)\n",
    "* Prompt with OpenAI (GPT-4) on Azure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import boto3\n",
    "# Azure imports\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azureml.fsspec import AzureMachineLearningFileSystem\n",
    "from requests_aws4auth import AWS4Auth\n",
    "# Haystack imports\n",
    "from haystack.document_stores import InMemoryDocumentStore, OpenSearchDocumentStore\n",
    "from haystack.nodes import (\n",
    "    QuestionGenerator,\n",
    "    EmbeddingRetriever, \n",
    "    BM25Retriever, \n",
    "    FARMReader, \n",
    "    PDFToTextConverter, \n",
    "    PreProcessor, \n",
    "    PromptModel,\n",
    "    PromptNode, \n",
    "    PromptTemplate, \n",
    "    AnswerParser\n",
    ")\n",
    "from haystack.pipelines import (\n",
    "    QuestionGenerationPipeline,\n",
    "    QuestionAnswerGenerationPipeline,\n",
    "    ExtractiveQAPipeline,\n",
    "    Pipeline\n",
    ")\n",
    "from haystack.schema import Document\n",
    "from haystack.utils import print_questions, print_answers\n",
    "\n",
    "# Huggingface transformers library\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from Azure storage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Azure workspace\n",
    "credential = DefaultAzureCredential()\n",
    "# Check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Machine Learning workspace details:\n",
    "subscription = '<subscription-id>'\n",
    "resource_group = '<resource_group>'\n",
    "workspace = '<workspace_name>'\n",
    "datastore_name = '<datastore_name>'\n",
    "\n",
    "# long-form Datastore uri format:\n",
    "uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate file system using following URI\n",
    "fs = AzureMachineLearningFileSystem(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View folder where data exists\n",
    "fs.ls('upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data asset for versioning and simple access by other services\n",
    "VERSION = '1'\n",
    "data_asset_name = '<DataAssetName>'\n",
    "file_path = f'azureml://datastores/{datastore_name}/paths/upload/<file_name>'\n",
    "# Define the Data asset object\n",
    "pdf_data = Data(\n",
    "    path=file_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"<description>\",\n",
    "    name=data_asset_name,\n",
    "    version=VERSION,\n",
    ")\n",
    "# Create the data asset in the workspace\n",
    "ml_client.data.create_or_update(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data asset \n",
    "data_versions = []\n",
    "data_assets = ml_client.data.list(name=data_asset_name)\n",
    "for asset in data_assets:\n",
    "    data_versions.append(asset.version)\n",
    "latest_version = max(data_versions)\n",
    "azure_ml_asset = ml_client.data.get(data_asset_name, version=latest_version)\n",
    "data_path_parts = Path(azure_ml_asset.path.split('datastores/')[1]).parts\n",
    "data_path = '/'.join(data_path_parts[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate in-memory document store if not using OpenSearch. In production, you would use a persistent vector database such as OpenSearch\n",
    "# document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access AWS opensearch\n",
    "region = '<region>'\n",
    "service = 'es' \n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service)\n",
    "host = \"<open_search_host_url>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start OpenSearch Document Store\n",
    "document_store = OpenSearchDocumentStore(\n",
    "    host = host,\n",
    "    port = 443,\n",
    "    aws4auth = awsauth,                                        \n",
    "    scheme = \"https\",\n",
    "    verify_certs = True,\n",
    "    username = None,\n",
    "    password = None,\n",
    "    similarity = 'cosine'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter for pdf to text\n",
    "converter = PDFToTextConverter(\n",
    "    remove_numeric_tables=True,\n",
    "    valid_languages=[\"en\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pdf to text\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "    fs.download(rpath=data_path, lpath=tempdir, recursive=False)\n",
    "    docs = converter.convert(file_path=Path(Path(tempdir) / \"<file_name>\"), meta=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess documents for efficient querying\n",
    "\n",
    "This is a default usage of the PreProcessor.\n",
    "Here, it performs cleaning of consecutive whitespaces\n",
    "and splits a single large document into smaller documents.\n",
    "https://docs.haystack.deepset.ai/docs/preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and chunk docs for more efficient retrieval\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "processed_docs = preprocessor.process(docs)\n",
    "print(f\"n_files_input: {len(docs)}\\nn_docs_output: {len(processed_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_docs[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write docs to document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To reset your DocumentStore\n",
    "\n",
    "# document_store.delete_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your documents into the DocumentStore\n",
    "document_store.write_documents(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for your documents\n",
    "embedding_retriever = EmbeddingRetriever(document_store = document_store,\n",
    "                               embedding_model=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_store.update_embeddings(embedding_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/A Pipeline (no Gen AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template of basic Haystack workflow \n",
    "\n",
    "# # Retriever: A Fast and simple algo to identify the most promising candidate documents\n",
    "retriever = BM25Retriever(document_store)\n",
    "\n",
    "# # Reader: Powerful but slower neural network trained for QA\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "reader = FARMReader(model_name)\n",
    "\n",
    "# # Pipeline: Combines all the components\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question without prompt node (no LLM/Gen AI) and with sparse retrieval\n",
    "question = \"<query>\"\n",
    "prediction = pipe.run(query=question)\n",
    "print_answers(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question using Embedding Retriever\n",
    "pipe = ExtractiveQAPipeline(reader, embedding_retriever)\n",
    "prediction = pipe.run(query=question)\n",
    "print_answers(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions\n",
    "Will take some time, especially with many documents in document store. Using GPU should speed up process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Question Generator\n",
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions from each document\n",
    "q_results = []\n",
    "reader = FARMReader(\"deepset/roberta-base-squad2\")\n",
    "q_pipeline = QuestionGenerationPipeline(question_generator)\n",
    "for idx, document in enumerate(tqdm(document_store)):\n",
    "    print(f\"\\n * Generating questions for document {idx} ...\\n\")\n",
    "    result = q_pipeline.run(documents=[document])\n",
    "    q_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions and answers from each document\n",
    "qag_results = []\n",
    "reader = FARMReader(\"deepset/roberta-base-squad2\")\n",
    "qag_pipeline = QuestionAnswerGenerationPipeline(question_generator, reader)\n",
    "for idx, document in enumerate(tqdm(document_store)):\n",
    "    print(f\"\\n * Generating questions and answers for document {idx}: {document.content}...\\n\")\n",
    "    result = qag_pipeline.run(documents=[document])\n",
    "    qag_results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM - Gen AI - text generation models from Huggingface\n",
    "https://docs.haystack.deepset.ai/docs/prompt_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.nodes import PromptNode\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'mosaicml/mpt-7b-instruct',\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Google Flan T5 (multi-task, text-to-text model) - HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '<query>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for your documents\n",
    "embedding_retriever = EmbeddingRetriever(document_store = document_store,\n",
    "                               embedding_model=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom supported prompt using PromptTemplate\n",
    "user_prompt = PromptTemplate(prompt=\"\"\"Synthesize a comprehensive answer from the following topk most relevant paragraphs and the given question. \n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the paragraphs. \n",
    "                             Your answer should be in your own words and be no longer than 100 words. \n",
    "                             \\n\\n Paragraphs: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "                             output_parser=AnswerParser(),) \n",
    "## \"mosaicml/mpt-7b-instruct\" as a larger example\n",
    "# Initiate the PromptNode \n",
    "prompt_node = PromptNode(\"google/flan-t5-small\", model_kwargs={\"model\":model, \"tokenizer\": tokenizer}, default_prompt_template=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=embedding_retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe.run(query=query, params={\"retriever\": {\"top_k\": 1}})\n",
    "[a.answer for a in output[\"answers\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"AZURE_API_KEY\")\n",
    "deployment_name = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "base_url = os.environ.get(\"AZURE_BASE_URL\")\n",
    "\n",
    "azure_prompt = PromptModel(\n",
    "    model_name_or_path=\"gpt-4\",\n",
    "    api_key=api_key,\n",
    "    model_kwargs={\n",
    "        \"api_version\": \"2023-07-01-preview\",\n",
    "        \"azure_deployment_name\": deployment_name,\n",
    "        \"azure_base_url\": base_url,\n",
    "        \"max_tokens\": 2000\n",
    "    },\n",
    ")\n",
    "azure_prompt_node = PromptNode(azure_prompt, default_prompt_template=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=embedding_retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=azure_prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe.run(query=query, params={\"retriever\": {\"top_k\": 5}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.answer for a in output[\"answers\"]][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
